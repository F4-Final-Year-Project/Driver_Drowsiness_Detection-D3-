{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d964ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 62340 images belonging to 2 classes.\n",
      "Found 15584 images belonging to 2 classes.\n",
      "Found 20834 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "1659/7792 [=====>........................] - ETA: 1:04:25 - loss: 0.2595 - accuracy: 0.8943"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout, Input, Flatten, Dense, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, CSVLogger, LearningRateScheduler\n",
    "\n",
    "# Check if a GPU is available\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    # Use the available GPU(s)\n",
    "    print(\"Training on GPU...\")\n",
    "    tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
    "\n",
    "# Data augmentation\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=30, shear_range=0.2, zoom_range=0.2, width_shift_range=0.2, height_shift_range=0.2, validation_split=0.2)\n",
    "\n",
    "# Load the training, validation, and test data\n",
    "batch_size = 8\n",
    "train_data = train_datagen.flow_from_directory(r'C:\\Users\\lenovo\\Documents\\Driver-Drowsiness-Detection-using-Deep-Learning\\prepareddata\\Train', target_size=(80, 80), batch_size=batch_size, class_mode='categorical', subset='training')\n",
    "val_data = train_datagen.flow_from_directory(r'C:\\Users\\lenovo\\Documents\\Driver-Drowsiness-Detection-using-Deep-Learning\\prepareddata\\Train', target_size=(80, 80), batch_size=batch_size, class_mode='categorical', subset='validation')\n",
    "test_data = ImageDataGenerator(rescale=1./255).flow_from_directory(r'C:\\Users\\lenovo\\Documents\\Driver-Drowsiness-Detection-using-Deep-Learning\\prepareddata\\Test', target_size=(80, 80), batch_size=batch_size, class_mode='categorical')\n",
    "\n",
    "# Load the InceptionV3 model\n",
    "base_model = InceptionV3(include_top=False, weights='imagenet', input_tensor=Input(shape=(80, 80, 3)))\n",
    "\n",
    "# Fine-tune the model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Add dense layers\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Define the callbacks\n",
    "checkpoint = ModelCheckpoint(r'C:\\Users\\lenovo\\Documents\\Driver-Drowsiness-Detection-using-Deep-Learning\\model\\model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=7, verbose=1, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, factor=0.1)\n",
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)\n",
    "csv_logger = CSVLogger('training.log', separator=',', append=False)\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 10\n",
    "history = model.fit(train_data, steps_per_epoch=train_data.samples // batch_size, epochs=epochs, validation_data=val_data, validation_steps=val_data.samples // batch_size, callbacks=[checkpoint, early_stop, reduce_lr, tensorboard, csv_logger, LearningRateScheduler(lr_schedule)])\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(test_data, verbose=1)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5529b856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
